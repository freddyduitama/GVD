{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.6.LSH.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddyduitama/GVD/blob/master/0_6_LSH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eVSCHzjLlZd9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Adecuacion de la plataforma**"
      ]
    },
    {
      "metadata": {
        "id": "alYd_PMvWeMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# instala el ambiente de spark..solo se corre una vez\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yvz9ToTpTgEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Configura variables de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zz22FmAIYIvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importa pyspark package\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjngtzOcT_QR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define  la sesion SPARK. \n",
        "conf = SparkConf().setAppName(\"ejemplo\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGaSqRZYUAjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Crea la sesión\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SKQSxEV1PHLF",
        "colab_type": "code",
        "outputId": "3c3e9130-2def-45e3-fd78-223ddd173b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# monta el google drive para usar sus archivos\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "usi63kydliJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Example 1.  Versión básica**"
      ]
    },
    {
      "metadata": {
        "id": "as8HsrKbA8_e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rNeCoC2QYUFF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dataframe dfA\n",
        "dataA = [(\"P0\", Vectors.dense([1.0, 1.0]),),\n",
        "         (\"P1\", Vectors.dense([1.0, -1.0]),),\n",
        "         (\"P2\", Vectors.dense([-1.0, -1.0]),),\n",
        "         (\"P3\", Vectors.dense([-1.0, 1.0]),)]\n",
        "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GeO1xI2rNbqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dataframe dfB\n",
        "dataB = [(\"P4\", Vectors.dense([1.0, 0.0]),),\n",
        "         (\"P5\", Vectors.dense([-1.0, 0.0]),),\n",
        "         (\"P6\", Vectors.dense([0.0, 1.0]),),\n",
        "         (\"P7\", Vectors.dense([0.0, -1.0]),)]\n",
        "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xx7up_-gdqBi",
        "colab_type": "code",
        "outputId": "77587e30-ff68-49ff-98be-a97d966df06a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "dfB.select(\"id\",\"features\").show(8,truncate=False)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+\n",
            "|id |features  |\n",
            "+---+----------+\n",
            "|P4 |[1.0,0.0] |\n",
            "|P5 |[-1.0,0.0]|\n",
            "|P6 |[0.0,1.0] |\n",
            "|P7 |[0.0,-1.0]|\n",
            "+---+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fHkcpAs4c3Eq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define Pipeline. bucketLenght define  number and size of buckets \n",
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
        "                                  numHashTables=1)\n",
        "model = brp.fit(dfA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pn24qqHldAGM",
        "colab_type": "code",
        "outputId": "fe3b6e1d-3628-44e8-8cbd-ae927f1a20e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Feature Transformation\n",
        "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
        "model.transform(dfA).sort(\"hashes\").show(8,truncate=False)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hashed dataset where hashed values are stored in the column 'hashes':\n",
            "+---+-----------+--------+\n",
            "|id |features   |hashes  |\n",
            "+---+-----------+--------+\n",
            "|P1 |[1.0,-1.0] |[[-2.0]]|\n",
            "|P2 |[-1.0,-1.0]|[[-1.0]]|\n",
            "|P0 |[1.0,1.0]  |[[0.0]] |\n",
            "|P3 |[-1.0,1.0] |[[1.0]] |\n",
            "+---+-----------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BPxtSudlRt7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Similarity join. "
      ]
    },
    {
      "metadata": {
        "id": "AzDMbu_Gf6Yb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
        "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
        "\n",
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
        "                                  numHashTables=1)\n",
        "model1 = brp.fit(dfA)\n",
        "model2 = brp.fit(dfB)\n",
        "\n",
        "dfAA=model1.transform(dfA).sort(\"hashes\")\n",
        "dfBB=model2.transform(dfB).sort(\"hashes\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFR2FgDAWQv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bf0d0f16-f952-4204-83af-5c1d40f41389"
      },
      "cell_type": "code",
      "source": [
        "print(\"join dfA and dfB on points having Euclidean distance smaller than 1.5:\")\n",
        "model.approxSimilarityJoin(dfAA, dfBB, 1.5, distCol=\"EuclideanDistance\")\\\n",
        "    .select(col(\"datasetA.id\").alias(\"idA\"), col(\"datasetA.features\").alias(\"coordinates\"),\n",
        "            col(\"datasetB.id\").alias(\"idB\"), col(\"datasetB.features\").alias(\"coordinates\"),\n",
        "            col(\"EuclideanDistance\")).sort(\"datasetA.id\").show(8,truncate=False)"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "join dfA and dfB on points having Euclidean distance smaller than 1.5:\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "|idA|coordinates|idB|coordinates|EuclideanDistance|\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "|P0 |[1.0,1.0]  |P6 |[0.0,1.0]  |1.0              |\n",
            "|P1 |[1.0,-1.0] |P4 |[1.0,0.0]  |1.0              |\n",
            "|P1 |[1.0,-1.0] |P7 |[0.0,-1.0] |1.0              |\n",
            "|P2 |[-1.0,-1.0]|P7 |[0.0,-1.0] |1.0              |\n",
            "|P3 |[-1.0,1.0] |P6 |[0.0,1.0]  |1.0              |\n",
            "|P3 |[-1.0,1.0] |P5 |[-1.0,0.0] |1.0              |\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjIZnenZU2--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#approximate nearest neighbor search."
      ]
    },
    {
      "metadata": {
        "id": "QBmnDrC6f_0U",
        "colab_type": "code",
        "outputId": "474e45ed-0d0f-4ebd-ce9d-a13d61a487a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
        "# neighbor search.\n",
        "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
        "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
        "\n",
        "# Reference point\n",
        "key = Vectors.dense([-1.0, 0.0])\n",
        "\n",
        "print(\"Approximately searching dfAA for 2 nearest neighbors of the key:\" , key)\n",
        "model.approxNearestNeighbors(dfAA, key, 2).show(8,truncate=False)"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Approximately searching dfAA for 2 nearest neighbors of the key: [-1.0,0.0]\n",
            "+---+----------+-------+----------------+\n",
            "|id |features  |hashes |distCol         |\n",
            "+---+----------+-------+----------------+\n",
            "|P3 |[-1.0,1.0]|[[0.0]]|1.0             |\n",
            "|P0 |[1.0,1.0] |[[0.0]]|2.23606797749979|\n",
            "+---+----------+-------+----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "19mELGxgRk_a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Second example using more points into the space"
      ]
    },
    {
      "metadata": {
        "id": "qDq7bBVrjOHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataC = [(\"P0\", Vectors.dense([1.0, 1.0]),),\n",
        "         (\"P1\", Vectors.dense([1.0, -1.0]),),\n",
        "         (\"P2\", Vectors.dense([-1.0, -1.0]),),\n",
        "         (\"P3\", Vectors.dense([-1.0, 1.0]),),\n",
        "         (\"P4\", Vectors.dense([1.0, 0.0]),),\n",
        "         (\"P5\", Vectors.dense([-1.0, 0.0]),),\n",
        "         (\"P6\", Vectors.dense([0.0, 1.0]),),\n",
        "         (\"P7\", Vectors.dense([2.0, 1.0]),),\n",
        "         (\"P8\", Vectors.dense([0.0, -1.0]),)]\n",
        "\n",
        "dfC = spark.createDataFrame(dataC, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPL2TbrssQpD",
        "colab_type": "code",
        "outputId": "02b50b79-dad2-409d-f3b0-d0b4e696198d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "dfC.select(\"id\",\"features\").show(9,truncate=False)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----------+\n",
            "|id |features   |\n",
            "+---+-----------+\n",
            "|P0 |[1.0,1.0]  |\n",
            "|P1 |[1.0,-1.0] |\n",
            "|P2 |[-1.0,-1.0]|\n",
            "|P3 |[-1.0,1.0] |\n",
            "|P4 |[1.0,0.0]  |\n",
            "|P5 |[-1.0,0.0] |\n",
            "|P6 |[0.0,1.0]  |\n",
            "|P7 |[2.0,1.0]  |\n",
            "|P8 |[0.0,-1.0] |\n",
            "+---+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LLe0ATxBsR5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=1.0,\n",
        "                                  numHashTables=1)\n",
        "model = brp.fit(dfC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yutbI5kTsWut",
        "colab_type": "code",
        "outputId": "3037f8f8-5518-4343-c8c3-db0122e761ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Feature Transformation\n",
        "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
        "model.transform(dfC).sort(\"hashes\").show(9,truncate=False)"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hashed dataset where hashed values are stored in the column 'hashes':\n",
            "+---+-----------+--------+\n",
            "|id |features   |hashes  |\n",
            "+---+-----------+--------+\n",
            "|P1 |[1.0,-1.0] |[[-2.0]]|\n",
            "|P7 |[2.0,1.0]  |[[-1.0]]|\n",
            "|P2 |[-1.0,-1.0]|[[-1.0]]|\n",
            "|P8 |[0.0,-1.0] |[[-1.0]]|\n",
            "|P4 |[1.0,0.0]  |[[-1.0]]|\n",
            "|P0 |[1.0,1.0]  |[[0.0]] |\n",
            "|P6 |[0.0,1.0]  |[[0.0]] |\n",
            "|P5 |[-1.0,0.0] |[[0.0]] |\n",
            "|P3 |[-1.0,1.0] |[[1.0]] |\n",
            "+---+-----------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}