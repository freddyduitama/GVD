{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.6.LSH.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddyduitama/GVD/blob/master/0_6_LSH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eVSCHzjLlZd9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Adecuacion de la plataforma**"
      ]
    },
    {
      "metadata": {
        "id": "alYd_PMvWeMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# instala el ambiente de spark..solo se corre una vez\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yvz9ToTpTgEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Configura variables de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zz22FmAIYIvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importa pyspark package\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGaSqRZYUAjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Crea la sesi√≥n\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19mELGxgRk_a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Example 1 : Class Slides"
      ]
    },
    {
      "metadata": {
        "id": "as8HsrKbA8_e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qDq7bBVrjOHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#shingles\n",
        "dataC = [(\"S1\", Vectors.dense([4.0, 1.0,2.0,0.0,0.0,1.0]),),\n",
        "              (\"S2\", Vectors.dense([1.0, 2.0,1.0,5.0,4.0,2.0]),),\n",
        "              (\"S3\", Vectors.dense([0.0,0.0, 1.0,1.0,3.0,0.0]),),\n",
        "              (\"S4\", Vectors.dense([1.0, 0.0,3.0,1.0,0.0,0.0]),),\n",
        "              (\"S5\", Vectors.dense([1.0, 0.0,0.0,0.0,1.0,1.0]),),\n",
        "              (\"S6\", Vectors.dense([0.0, 0.0,1.0,1.0,3.0,1.0]),)]\n",
        "\n",
        "dfC = spark.createDataFrame(dataC, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLe0ATxBsR5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define Pipeline\n",
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0, numHashTables=1)\n",
        "model = brp.fit(dfC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPL2TbrssQpD",
        "colab_type": "code",
        "outputId": "2676aca6-6c49-4e4a-f87e-b48887eb2f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "dfC.select(\"id\",\"features\").show(9,truncate=False)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------------+\n",
            "|id |features                 |\n",
            "+---+-------------------------+\n",
            "|S1 |[4.0,1.0,2.0,0.0,0.0,1.0]|\n",
            "|S2 |[1.0,2.0,1.0,5.0,4.0,2.0]|\n",
            "|S3 |[0.0,0.0,1.0,1.0,3.0,0.0]|\n",
            "|S4 |[1.0,0.0,3.0,1.0,0.0,0.0]|\n",
            "|S5 |[1.0,0.0,0.0,0.0,1.0,1.0]|\n",
            "|S6 |[0.0,0.0,1.0,1.0,3.0,1.0]|\n",
            "+---+-------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yutbI5kTsWut",
        "colab_type": "code",
        "outputId": "fdbd7a64-64e9-47e7-d1f9-24a5d974f0f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# Feature Transformation\n",
        "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
        "model.transform(dfC).sort(\"hashes\").show(9,truncate=False)"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hashed dataset where hashed values are stored in the column 'hashes':\n",
            "+---+-------------------------+--------+\n",
            "|id |features                 |hashes  |\n",
            "+---+-------------------------+--------+\n",
            "|S1 |[4.0,1.0,2.0,0.0,0.0,1.0]|[[-1.0]]|\n",
            "|S5 |[1.0,0.0,0.0,0.0,1.0,1.0]|[[-1.0]]|\n",
            "|S4 |[1.0,0.0,3.0,1.0,0.0,0.0]|[[-1.0]]|\n",
            "|S6 |[0.0,0.0,1.0,1.0,3.0,1.0]|[[0.0]] |\n",
            "|S3 |[0.0,0.0,1.0,1.0,3.0,0.0]|[[0.0]] |\n",
            "|S2 |[1.0,2.0,1.0,5.0,4.0,2.0]|[[1.0]] |\n",
            "+---+-------------------------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "usi63kydliJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Example 2.  naive example**"
      ]
    },
    {
      "metadata": {
        "id": "rNeCoC2QYUFF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dataframe dfA\n",
        "dataA = [(\"P0\", Vectors.dense([1.0, 1.0]),),\n",
        "         (\"P1\", Vectors.dense([1.0, -1.0]),),\n",
        "         (\"P2\", Vectors.dense([-1.0, -1.0]),),\n",
        "         (\"P3\", Vectors.dense([-1.0, 1.0]),)]\n",
        "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHkcpAs4c3Eq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define Pipeline. bucketLenght define  number and size of buckets \n",
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
        "                                  numHashTables=1)\n",
        "model = brp.fit(dfA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pn24qqHldAGM",
        "colab_type": "code",
        "outputId": "d2c0f181-d178-4a41-e124-e4d76389363c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Feature Transformation\n",
        "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
        "model.transform(dfA).sort(\"hashes\").show(8,truncate=False)"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hashed dataset where hashed values are stored in the column 'hashes':\n",
            "+---+-----------+--------+\n",
            "|id |features   |hashes  |\n",
            "+---+-----------+--------+\n",
            "|P2 |[-1.0,-1.0]|[[-1.0]]|\n",
            "|P1 |[1.0,-1.0] |[[-1.0]]|\n",
            "|P3 |[-1.0,1.0] |[[0.0]] |\n",
            "|P0 |[1.0,1.0]  |[[0.0]] |\n",
            "+---+-----------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BPxtSudlRt7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Similarity join. "
      ]
    },
    {
      "metadata": {
        "id": "GeO1xI2rNbqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dataframe dfB\n",
        "dataB = [(\"P4\", Vectors.dense([1.0, 0.0]),),\n",
        "              (\"P5\", Vectors.dense([-1.0, 0.0]),),\n",
        "              (\"P6\", Vectors.dense([0.0, 1.0]),),\n",
        "              (\"P7\", Vectors.dense([0.0, -1.0]),)]\n",
        "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AzDMbu_Gf6Yb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We compute hashes. Next step will use  the already-transformed dataset\n",
        "\n",
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
        "                                  numHashTables=1)\n",
        "model1 = brp.fit(dfA)\n",
        "model2 = brp.fit(dfB)\n",
        "\n",
        "hash_dfA=model1.transform(dfA).sort(\"hashes\")\n",
        "hash_dfB=model2.transform(dfB).sort(\"hashes\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFR2FgDAWQv9",
        "colab_type": "code",
        "outputId": "44122d6d-5421-4372-f8b4-38f5426ed067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"join dfA and dfB on points having Euclidean distance smaller than 1.5:\")\n",
        "\n",
        "model.approxSimilarityJoin(hash_dfA, hash_dfB, 1.5, distCol=\"EuclideanDistance\")\\\n",
        "          .select(col(\"datasetA.id\").alias(\"idA\"), col(\"datasetA.features\").alias(\"coordinates\"),\n",
        "                     col(\"datasetB.id\").alias(\"idB\"), col(\"datasetB.features\").alias(\"coordinates\"),\n",
        "                     col(\"EuclideanDistance\")).sort(\"datasetA.id\").show(8,truncate=False)"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "join dfA and dfB on points having Euclidean distance smaller than 1.5:\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "|idA|coordinates|idB|coordinates|EuclideanDistance|\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "|P0 |[1.0,1.0]  |P6 |[0.0,1.0]  |1.0              |\n",
            "|P1 |[1.0,-1.0] |P4 |[1.0,0.0]  |1.0              |\n",
            "|P1 |[1.0,-1.0] |P7 |[0.0,-1.0] |1.0              |\n",
            "|P2 |[-1.0,-1.0]|P7 |[0.0,-1.0] |1.0              |\n",
            "|P3 |[-1.0,1.0] |P6 |[0.0,1.0]  |1.0              |\n",
            "|P3 |[-1.0,1.0] |P5 |[-1.0,0.0] |1.0              |\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjIZnenZU2--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Approximate nearest neighbor search."
      ]
    },
    {
      "metadata": {
        "id": "2TPxBOcuBMJn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataD = [(\"P0\", Vectors.dense([1.0, 1.0]),),\n",
        "              (\"P1\", Vectors.dense([1.0, -1.0]),),\n",
        "              (\"P2\", Vectors.dense([-1.0, -1.0]),),\n",
        "              (\"P3\", Vectors.dense([-1.0, 1.0]),),\n",
        "              (\"P4\", Vectors.dense([1.0, 0.0]),),\n",
        "              (\"P5\", Vectors.dense([-1.0, 0.0]),),\n",
        "              (\"P6\", Vectors.dense([0.0, 1.0]),),\n",
        "              (\"P7\", Vectors.dense([0.0, -1.0]),)]\n",
        "\n",
        "dfD = spark.createDataFrame(dataD, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBmnDrC6f_0U",
        "colab_type": "code",
        "outputId": "fd3ab814-28a5-4917-8ef3-0eb15a357bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "# Perform approximate nearest neighbor search.\n",
        "# We  passe the already-transformed dataset\n",
        "\n",
        "# Reference point\n",
        "key = Vectors.dense([0.5, -0.5])\n",
        "\n",
        "print(\"Approximately searching dfC for 3 nearest neighbors of the key:\" , key)\n",
        "model.approxNearestNeighbors(dfD, key, 3).show(8,truncate=False)"
      ],
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Approximately searching dfC for 3 nearest neighbors of the key: [0.5,-0.5]\n",
            "+---+----------+--------+------------------+\n",
            "|id |features  |hashes  |distCol           |\n",
            "+---+----------+--------+------------------+\n",
            "|P1 |[1.0,-1.0]|[[-1.0]]|0.7071067811865476|\n",
            "|P4 |[1.0,0.0] |[[-1.0]]|0.7071067811865476|\n",
            "|P7 |[0.0,-1.0]|[[-1.0]]|0.7071067811865476|\n",
            "+---+----------+--------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}