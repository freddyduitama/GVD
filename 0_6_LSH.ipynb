{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.6.LSH.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddyduitama/GVD/blob/master/0_6_LSH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eVSCHzjLlZd9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Adecuacion de la plataforma**"
      ]
    },
    {
      "metadata": {
        "id": "alYd_PMvWeMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# instala el ambiente de spark..solo se corre una vez\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yvz9ToTpTgEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Configura variables de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zz22FmAIYIvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importa pyspark package\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGaSqRZYUAjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Crea la sesi√≥n\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SKQSxEV1PHLF",
        "colab_type": "code",
        "outputId": "3c3e9130-2def-45e3-fd78-223ddd173b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# monta el google drive para usar sus archivos\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "usi63kydliJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Example 1.  naive example**"
      ]
    },
    {
      "metadata": {
        "id": "as8HsrKbA8_e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rNeCoC2QYUFF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dataframe dfA\n",
        "dataA = [(\"P0\", Vectors.dense([1.0, 1.0]),),\n",
        "         (\"P1\", Vectors.dense([1.0, -1.0]),),\n",
        "         (\"P2\", Vectors.dense([-1.0, -1.0]),),\n",
        "         (\"P3\", Vectors.dense([-1.0, 1.0]),)]\n",
        "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GeO1xI2rNbqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dataframe dfB\n",
        "dataB = [(\"P4\", Vectors.dense([1.0, 0.0]),),\n",
        "         (\"P5\", Vectors.dense([-1.0, 0.0]),),\n",
        "         (\"P6\", Vectors.dense([0.0, 1.0]),),\n",
        "         (\"P7\", Vectors.dense([0.0, -1.0]),)]\n",
        "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xx7up_-gdqBi",
        "colab_type": "code",
        "outputId": "a46dc22e-98aa-4065-d2df-b70bb9ec440a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "dfB.select(\"id\",\"features\").show(8,truncate=False)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+\n",
            "|id |features  |\n",
            "+---+----------+\n",
            "|P4 |[1.0,0.0] |\n",
            "|P5 |[-1.0,0.0]|\n",
            "|P6 |[0.0,1.0] |\n",
            "|P7 |[0.0,-1.0]|\n",
            "+---+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fHkcpAs4c3Eq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define Pipeline. bucketLenght define  number and size of buckets \n",
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
        "                                  numHashTables=1)\n",
        "model = brp.fit(dfA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pn24qqHldAGM",
        "colab_type": "code",
        "outputId": "d18975c1-40ce-4f65-a6ac-5d8150d5311a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Feature Transformation\n",
        "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
        "model.transform(dfA).sort(\"hashes\").show(8,truncate=False)"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hashed dataset where hashed values are stored in the column 'hashes':\n",
            "+---+-----------+--------+\n",
            "|id |features   |hashes  |\n",
            "+---+-----------+--------+\n",
            "|P2 |[-1.0,-1.0]|[[-1.0]]|\n",
            "|P1 |[1.0,-1.0] |[[-1.0]]|\n",
            "|P3 |[-1.0,1.0] |[[0.0]] |\n",
            "|P0 |[1.0,1.0]  |[[0.0]] |\n",
            "+---+-----------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BPxtSudlRt7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Similarity join. "
      ]
    },
    {
      "metadata": {
        "id": "AzDMbu_Gf6Yb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We avoid computing hashes by passing in the already-transformed dataset\n",
        "\n",
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
        "                                  numHashTables=1)\n",
        "model1 = brp.fit(dfA)\n",
        "model2 = brp.fit(dfB)\n",
        "\n",
        "dfAA=model1.transform(dfA).sort(\"hashes\")\n",
        "dfBB=model2.transform(dfB).sort(\"hashes\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFR2FgDAWQv9",
        "colab_type": "code",
        "outputId": "a208e110-f4cc-480f-d6db-290195a1ba7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"join dfA and dfB on points having Euclidean distance smaller than 1.5:\")\n",
        "model.approxSimilarityJoin(dfAA, dfBB, 1.5, distCol=\"EuclideanDistance\")\\\n",
        "    .select(col(\"datasetA.id\").alias(\"idA\"), col(\"datasetA.features\").alias(\"coordinates\"),\n",
        "            col(\"datasetB.id\").alias(\"idB\"), col(\"datasetB.features\").alias(\"coordinates\"),\n",
        "            col(\"EuclideanDistance\")).sort(\"datasetA.id\").show(8,truncate=False)"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "join dfA and dfB on points having Euclidean distance smaller than 1.5:\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "|idA|coordinates|idB|coordinates|EuclideanDistance|\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "|P0 |[1.0,1.0]  |P6 |[0.0,1.0]  |1.0              |\n",
            "|P1 |[1.0,-1.0] |P4 |[1.0,0.0]  |1.0              |\n",
            "|P1 |[1.0,-1.0] |P7 |[0.0,-1.0] |1.0              |\n",
            "|P2 |[-1.0,-1.0]|P7 |[0.0,-1.0] |1.0              |\n",
            "|P3 |[-1.0,1.0] |P6 |[0.0,1.0]  |1.0              |\n",
            "|P3 |[-1.0,1.0] |P5 |[-1.0,0.0] |1.0              |\n",
            "+---+-----------+---+-----------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjIZnenZU2--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Approximate nearest neighbor search."
      ]
    },
    {
      "metadata": {
        "id": "QBmnDrC6f_0U",
        "colab_type": "code",
        "outputId": "bfe55642-bdd6-4d17-d232-33362ed3d416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
        "# neighbor search.\n",
        "# We  avoid computing hashes by passing in the already-transformed dataset\n",
        "\n",
        "# Reference point\n",
        "key = Vectors.dense([-1.0, 0.0])\n",
        "\n",
        "print(\"Approximately searching dfAA for 2 nearest neighbors of the key:\" , key)\n",
        "model.approxNearestNeighbors(dfAA, key, 2).show(8,truncate=False)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Approximately searching dfAA for 2 nearest neighbors of the key: [-1.0,0.0]\n",
            "+---+----------+-------+----------------+\n",
            "|id |features  |hashes |distCol         |\n",
            "+---+----------+-------+----------------+\n",
            "|P3 |[-1.0,1.0]|[[0.0]]|1.0             |\n",
            "|P0 |[1.0,1.0] |[[0.0]]|2.23606797749979|\n",
            "+---+----------+-------+----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "19mELGxgRk_a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class Slides example"
      ]
    },
    {
      "metadata": {
        "id": "qDq7bBVrjOHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# documents shingling \n",
        "dataC = [(\"S1\", Vectors.dense([4.0, 1.0,2.0,0.0,0.0,1.0]),),\n",
        "              (\"S2\", Vectors.dense([1.0, 2.0,1.0,5.0,4.0,2.0]),),\n",
        "              (\"S3\", Vectors.dense([0.0,0.0, 1.0,1.0,3.0,0.0]),),\n",
        "              (\"S4\", Vectors.dense([1.0, 0.0,3.0,1.0,0.0,0.0]),),\n",
        "              (\"S5\", Vectors.dense([1.0, 0.0,0.0,0.0,1.0,1.0]),),\n",
        "              (\"S6\", Vectors.dense([0.0, 0.0,1.0,1.0,3.0,1.0]),)]\n",
        "\n",
        "dfC = spark.createDataFrame(dataC, [\"id\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPL2TbrssQpD",
        "colab_type": "code",
        "outputId": "d8bc0c0d-ac61-4dda-b225-a750e257bdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "dfC.select(\"id\",\"features\").show(9,truncate=False)"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------------+\n",
            "|id |features                 |\n",
            "+---+-------------------------+\n",
            "|S1 |[4.0,1.0,2.0,0.0,0.0,1.0]|\n",
            "|S2 |[1.0,2.0,1.0,5.0,4.0,2.0]|\n",
            "|S3 |[0.0,0.0,1.0,1.0,3.0,0.0]|\n",
            "|S4 |[1.0,0.0,3.0,1.0,0.0,0.0]|\n",
            "|S5 |[1.0,0.0,0.0,0.0,1.0,1.0]|\n",
            "|S6 |[0.0,0.0,1.0,1.0,3.0,1.0]|\n",
            "+---+-------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LLe0ATxBsR5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define Pipeline\n",
        "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=3.0,\n",
        "                                  numHashTables=1)\n",
        "model = brp.fit(dfC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yutbI5kTsWut",
        "colab_type": "code",
        "outputId": "cb4b5ae1-2701-4eef-e917-83eb7888436f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# Feature Transformation\n",
        "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
        "model.transform(dfC).sort(\"hashes\").show(9,truncate=False)"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hashed dataset where hashed values are stored in the column 'hashes':\n",
            "+---+-------------------------+--------+\n",
            "|id |features                 |hashes  |\n",
            "+---+-------------------------+--------+\n",
            "|S1 |[4.0,1.0,2.0,0.0,0.0,1.0]|[[-1.0]]|\n",
            "|S5 |[1.0,0.0,0.0,0.0,1.0,1.0]|[[-1.0]]|\n",
            "|S4 |[1.0,0.0,3.0,1.0,0.0,0.0]|[[-1.0]]|\n",
            "|S6 |[0.0,0.0,1.0,1.0,3.0,1.0]|[[0.0]] |\n",
            "|S3 |[0.0,0.0,1.0,1.0,3.0,0.0]|[[0.0]] |\n",
            "|S2 |[1.0,2.0,1.0,5.0,4.0,2.0]|[[1.0]] |\n",
            "+---+-------------------------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}